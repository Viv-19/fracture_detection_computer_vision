{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21636c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting hyperparameter tuning...\n",
      "Reloading Tuner from tuner_logs\\densenet_fracture\\tuner0.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 230\u001b[0m\n\u001b[0;32m    227\u001b[0m X_train, y_train, X_val, y_val \u001b[38;5;241m=\u001b[39m load_metadata()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Step 2: Hyperparameter tuning using small subset\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m best_hp \u001b[38;5;241m=\u001b[39m \u001b[43mtune_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# Step 3: Train final model on full data with best hyperparameters\u001b[39;00m\n\u001b[0;32m    233\u001b[0m history \u001b[38;5;241m=\u001b[39m train_final_model(best_hp, X_train, y_train, X_val, y_val)\n",
      "Cell \u001b[1;32mIn[3], line 142\u001b[0m, in \u001b[0;36mtune_hyperparameters\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m    132\u001b[0m tuner \u001b[38;5;241m=\u001b[39m RandomSearch(\n\u001b[0;32m    133\u001b[0m     build_model,\n\u001b[0;32m    134\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensenet_fracture\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Run the tuner\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(train_gen, validation_data\u001b[38;5;241m=\u001b[39m(\u001b[43mval_gen\u001b[49m), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Return the best hyperparameters\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_gen' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------\n",
    "\n",
    "# Set all config in one dictionary for easy access\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'train_dir': r'D:\\collage project\\fracture detection comp vision\\fracture_detection_computer_vision\\src\\components\\data\\processed\\train',\n",
    "        'val_dir': r'D:\\collage project\\fracture detection comp vision\\fracture_detection_computer_vision\\src\\components\\data\\processed\\valid',\n",
    "        'metadata_path': r'D:\\collage project\\fracture detection comp vision\\fracture_detection_computer_vision\\src\\components\\data\\processed\\processed_metadata.csv',\n",
    "        'image_size': (224, 224),  # Input size expected by DenseNet121\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': 10  # Can increase after initial testing\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# DATA LOADER (Custom Generator)\n",
    "# ----------------------------\n",
    "\n",
    "class NPYDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator that reads preprocessed image arrays stored in .npy format\n",
    "    Useful for loading data dynamically without keeping everything in memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, labels, batch_size=32, shuffle=True):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Total number of batches per epoch\n",
    "        return int(np.ceil(len(self.paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Generate indices for the batch\n",
    "        batch_paths = self.paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Load image arrays\n",
    "        batch_images = [np.load(p) for p in batch_paths]\n",
    "        return np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle data at the end of each epoch\n",
    "        if self.shuffle:\n",
    "            idx = np.arange(len(self.paths))\n",
    "            np.random.shuffle(idx)\n",
    "            self.paths = self.paths[idx]\n",
    "            self.labels = self.labels[idx]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD METADATA\n",
    "# ----------------------------\n",
    "\n",
    "def load_metadata():\n",
    "    \"\"\"\n",
    "    Reads the CSV metadata and separates paths/labels into training and validation sets.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(CONFIG['data']['metadata_path'])\n",
    "    train_df = df[df['path'].str.contains('train')]\n",
    "    val_df = df[df['path'].str.contains('valid')]\n",
    "    return train_df['path'].values, train_df['label'].values, val_df['path'].values, val_df['label'].values\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MODEL BUILDER FUNCTION (used by Keras Tuner)\n",
    "# ----------------------------\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Builds the transfer learning model using DenseNet121 as base.\n",
    "    Hyperparameters are tuned using Keras Tuner.\n",
    "    \"\"\"\n",
    "    # Load pretrained DenseNet121 model without the top (FC) layer\n",
    "    base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(*CONFIG['data']['image_size'], 3))\n",
    "\n",
    "    # Freeze base model to prevent updating its weights during initial training\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add custom classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)  # Converts 7x7x1024 to 1024\n",
    "    x = Dense(hp.Int('dense_units', min_value=128, max_value=512, step=64), activation='relu')(x)\n",
    "    x = Dropout(hp.Float('dropout_rate', 0.3, 0.7, step=0.1))(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "\n",
    "    # Compile the model with selected learning rate\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Choice('learning_rate', [1e-4, 1e-5])),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# HYPERPARAMETER TUNING\n",
    "# ----------------------------\n",
    "\n",
    "def tune_hyperparameters(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter search using a subset of the training data.\n",
    "    \"\"\"\n",
    "    print(\"üîç Starting hyperparameter tuning...\")\n",
    "\n",
    "    # Use a small fraction of training data to speed up tuning\n",
    "    X_sub, _, y_sub, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=42)\n",
    "    train_gen = NPYDataGenerator(X_sub, y_sub, batch_size=CONFIG['data']['batch_size'])\n",
    "\n",
    "    # Create tuner\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_auc',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=1,\n",
    "        directory='tuner_logs',\n",
    "        project_name='densenet_fracture'\n",
    "    )\n",
    "\n",
    "    # Create training and validation generators\n",
    "    train_gen = NPYDataGenerator(X_train, y_train, batch_size=32, augment=True)\n",
    "    val_gen = NPYDataGenerator(X_val, y_val, batch_size=32, augment=False)\n",
    "\n",
    "\n",
    "    # Run the tuner\n",
    "    tuner.search(train_gen, validation_data=(val_gen), epochs=3)\n",
    "\n",
    "\n",
    "    # Return the best hyperparameters\n",
    "    return tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# FINAL TRAINING\n",
    "# ----------------------------\n",
    "\n",
    "def train_final_model(hp, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Train the final model using the best hyperparameters on the full dataset.\n",
    "    \"\"\"\n",
    "    model = build_model(hp)\n",
    "\n",
    "    # Initialize data generators\n",
    "    train_gen = NPYDataGenerator(X_train, y_train, batch_size=CONFIG['data']['batch_size'])\n",
    "    val_gen = NPYDataGenerator(X_val, y_val, batch_size=CONFIG['data']['batch_size'], shuffle=False)\n",
    "\n",
    "    # Callbacks to save best model and stop early if needed\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_auc', patience=3, mode='max', restore_best_weights=True),\n",
    "        ModelCheckpoint('best_densenet_model.h5', monitor='val_auc', save_best_only=True, mode='max')\n",
    "    ]\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=CONFIG['training']['epochs'],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    print(\"‚úÖ Final evaluation on validation set:\")\n",
    "    loss, acc, auc = model.evaluate(val_gen)\n",
    "    print(f\"Validation Loss: {loss:.4f}, Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    # Save final model\n",
    "    model.save(\"final_densenet_fracture_model.h5\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# PLOTTING\n",
    "# ----------------------------\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation curves for accuracy and AUC.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot AUC\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['auc'], label='Train AUC')\n",
    "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "    plt.title(\"Training vs Validation AUC\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curves_densenet.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN PIPELINE\n",
    "# ----------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Step 1: Load file paths and labels\n",
    "    X_train, y_train, X_val, y_val = load_metadata()\n",
    "\n",
    "    # Step 2: Hyperparameter tuning using small subset\n",
    "    best_hp = tune_hyperparameters(X_train, y_train)\n",
    "\n",
    "    # Step 3: Train final model on full data with best hyperparameters\n",
    "    history = train_final_model(best_hp, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Step 4: Visualize training progress\n",
    "    plot_training_curves(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89924688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
