{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "371600ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MURA Path-Label Collector (YAML Config Version)\n",
    "-----------------------------------------------\n",
    "This script reads configuration from a YAML file (data_config.yaml)\n",
    "and collects image paths and corresponding labels into a CSV file.\n",
    "\"\"\"\n",
    "\n",
    "# ---- Import Required Libraries ----\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5c2849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Embedded Configuration ----\n",
    "CONFIG_YAML = \"\"\"\n",
    "data:\n",
    "  base_path: \"D:/collage project/fracture detection comp vision/MURA-v1.1\"\n",
    "  subsets: [\"train\", \"valid\"]\n",
    "  body_parts: [\n",
    "    \"XR_ELBOW\",\n",
    "    \"XR_FINGER\",\n",
    "    \"XR_FOREARM\",\n",
    "    \"XR_HAND\",\n",
    "    \"XR_HUMERUS\",\n",
    "    \"XR_SHOULDER\",\n",
    "    \"XR_WRIST\"\n",
    "  ]\n",
    "  metadata_output: \"data/processed/metadata.csv\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e9dab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Modified Function to Load Configurations ----\n",
    "def load_config(config_path=None):\n",
    "    \"\"\"\n",
    "    Load and validate configuration settings from embedded YAML.\n",
    "    (Modified to use embedded config if no path provided)\n",
    "    \"\"\"\n",
    "    if config_path:\n",
    "        # If path provided, load from file (original behavior)\n",
    "        with open(config_path) as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    else:\n",
    "        # Use embedded config\n",
    "        config = yaml.safe_load(CONFIG_YAML)\n",
    "    \n",
    "    # Check if all required fields exist\n",
    "    required_fields = ['base_path', 'subsets', 'body_parts', 'metadata_output']\n",
    "    for field in required_fields:\n",
    "        if field not in config['data']:\n",
    "            raise ValueError(f\"Missing required config field: data.{field}\")\n",
    "    \n",
    "    return config['data']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "059bcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Function to Collect Image Paths and Labels ----\n",
    "def collect_paths_labels(config):\n",
    "    \"\"\"\n",
    "    Traverse dataset folders to collect image paths and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary from load_config().\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing 'path' and 'label' columns.\n",
    "    \"\"\"\n",
    "    paths = []   # Store image file paths\n",
    "    labels = []  # Store corresponding labels (0 = negative, 1 = positive)\n",
    "    \n",
    "    print(f\"Collecting paths from {config['base_path']}...\")\n",
    "\n",
    "    # Loop over train/valid subsets\n",
    "    for subset in config['subsets']:\n",
    "        subset_path = os.path.join(config['base_path'], subset)\n",
    "        \n",
    "        if not os.path.exists(subset_path):\n",
    "            print(f\"Warning: {subset_path} not found. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Loop over each body part\n",
    "        for body_part in config['body_parts']:\n",
    "            body_part_path = os.path.join(subset_path, body_part)\n",
    "            \n",
    "            if not os.path.exists(body_part_path):\n",
    "                continue\n",
    "\n",
    "            # Get all patients under a body part\n",
    "            patients = [d for d in os.listdir(body_part_path) \n",
    "                        if os.path.isdir(os.path.join(body_part_path, d))]\n",
    "\n",
    "            # Loop over patients with a progress bar\n",
    "            for patient in tqdm(patients, desc=f\"{subset}/{body_part}\"):\n",
    "                patient_path = os.path.join(body_part_path, patient)\n",
    "\n",
    "                # Get all studies (positive/negative) for this patient\n",
    "                studies = [d for d in os.listdir(patient_path) \n",
    "                           if os.path.isdir(os.path.join(patient_path, d))]\n",
    "\n",
    "                # Loop over studies\n",
    "                for study in studies:\n",
    "                    study_path = os.path.join(patient_path, study)\n",
    "\n",
    "                    # Determine label based on folder name\n",
    "                    label = 0 if 'negative' in study.lower() else 1\n",
    "\n",
    "                    # Loop over images inside the study folder\n",
    "                    for img_file in os.listdir(study_path):\n",
    "                        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            # Append full path and label\n",
    "                            paths.append(os.path.join(study_path, img_file))\n",
    "                            labels.append(label)\n",
    "\n",
    "    # Convert collected data into a DataFrame\n",
    "    return pd.DataFrame({'path': paths, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e396bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Function to Save Metadata to CSV ----\n",
    "def save_metadata(df, output_path):\n",
    "    \"\"\"\n",
    "    Save the collected paths and labels to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'path' and 'label' columns.\n",
    "        output_path (str): Path where the CSV file will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Saved {len(df)} records to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27259ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paths from D:/collage project/fracture detection comp vision/MURA-v1.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/XR_ELBOW: 100%|██████████| 1711/1711 [00:00<00:00, 7306.70it/s]\n",
      "train/XR_FINGER: 100%|██████████| 1865/1865 [00:00<00:00, 7630.58it/s]\n",
      "train/XR_FOREARM: 100%|██████████| 865/865 [00:00<00:00, 8884.91it/s]\n",
      "train/XR_HAND: 100%|██████████| 1945/1945 [00:00<00:00, 7913.39it/s]\n",
      "train/XR_HUMERUS: 100%|██████████| 587/587 [00:00<00:00, 9339.88it/s]\n",
      "train/XR_SHOULDER: 100%|██████████| 2694/2694 [00:00<00:00, 7661.64it/s]\n",
      "train/XR_WRIST: 100%|██████████| 3267/3267 [00:00<00:00, 8171.72it/s]\n",
      "valid/XR_ELBOW: 100%|██████████| 152/152 [00:00<00:00, 5871.35it/s]\n",
      "valid/XR_FINGER: 100%|██████████| 166/166 [00:00<00:00, 8064.10it/s]\n",
      "valid/XR_FOREARM: 100%|██████████| 129/129 [00:00<00:00, 7170.42it/s]\n",
      "valid/XR_HAND: 100%|██████████| 159/159 [00:00<00:00, 8682.84it/s]\n",
      "valid/XR_HUMERUS: 100%|██████████| 132/132 [00:00<00:00, 5463.60it/s]\n",
      "valid/XR_SHOULDER: 100%|██████████| 173/173 [00:00<00:00, 5646.72it/s]\n",
      "valid/XR_WRIST: 100%|██████████| 207/207 [00:00<00:00, 5646.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 40009 records to data/processed/metadata.csv\n",
      "\n",
      "📊 Label Distribution:\n",
      "label\n",
      "0    23606\n",
      "1    16403\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🔎 Sample Records:\n",
      "                                                path  label\n",
      "0  D:/collage project/fracture detection comp vis...      0\n",
      "1  D:/collage project/fracture detection comp vis...      0\n",
      "2  D:/collage project/fracture detection comp vis...      0\n"
     ]
    }
   ],
   "source": [
    "# ---- MAIN EXECUTION BLOCK ----\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Step 1: Load YAML configuration\n",
    "        config = load_config()\n",
    "\n",
    "        # Step 2: Collect paths and labels based on config\n",
    "        metadata_df = collect_paths_labels(config)\n",
    "\n",
    "        # Step 3: Save metadata to a CSV file\n",
    "        save_metadata(metadata_df, config['metadata_output'])\n",
    "\n",
    "        # Step 4: Print some dataset insights\n",
    "        print(\"\\n📊 Label Distribution:\")\n",
    "        print(metadata_df['label'].value_counts())\n",
    "\n",
    "        print(\"\\n🔎 Sample Records:\")\n",
    "        print(metadata_df.head(3))\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors during execution\n",
    "        print(f\"❌ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c2cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47caa31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
